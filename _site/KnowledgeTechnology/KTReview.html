<!DOCTYPE html>
<html lang="en-US">
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <head>

    
    <meta charset="UTF-8">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Knowledge Technology | IT Taolu</title>
<meta name="generator" content="Jekyll v3.7.4" />
<meta property="og:title" content="Knowledge Technology" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="http://localhost:4000/KnowledgeTechnology/KTReview.html" />
<meta property="og:url" content="http://localhost:4000/KnowledgeTechnology/KTReview.html" />
<meta property="og:site_name" content="IT Taolu" />
<script type="application/ld+json">
{"@type":"WebPage","url":"http://localhost:4000/KnowledgeTechnology/KTReview.html","headline":"Knowledge Technology","@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <link rel="stylesheet" href="/assets/css/style.css?v=86b26a9bc73ec8ac393d17082d6973f031b1a886">
  </head>
  <body>
    <a id="skip-to-content" href="#content">Skip to the content.</a>

    <header class="page-header" role="banner">
      <h1 class="project-name">IT Taolu</h1>
      <h2 class="project-tagline"></h2>
      
        <a href="http://github.com/wylswz/wylswz.github.io" class="btn">View on GitHub</a>
      
      
    </header>

    <main id="content" class="main-content" role="main">
      
<h1 id="knowledge-technology">Knowledge Technology</h1>
<ul>
  <li><a href="#knowledge-technology">Knowledge Technology</a>
    <ul>
      <li><a href="#basic-concepts">Basic Concepts</a></li>
      <li><a href="#document-representation">Document representation</a></li>
      <li><a href="#string-processing">String processing</a>
        <ul>
          <li><a href="#regular-expression">Regular expression</a></li>
        </ul>
      </li>
      <li><a href="#similarity-of-text-documents">Similarity (of text documents)</a>
        <ul>
          <li><a href="#terminologies">Terminologies</a></li>
          <li><a href="#tf-idf">TF-IDF</a></li>
          <li><a href="#jaccard-similarity">Jaccard Similarity</a></li>
          <li><a href="#dice-similarity">Dice Similarity</a></li>
          <li><a href="#cosine-distance">Cosine Distance</a></li>
          <li><a href="#relative-entropy-kullback-leibler-deivergence">Relative entropy (Kullback-Leibler deivergence)</a></li>
          <li><a href="#skew-divergence">Skew divergence</a></li>
          <li><a href="#jensen-shannon-divergence">Jensen-Shannon divergence</a></li>
        </ul>
      </li>
      <li><a href="#probability">Probability</a>
        <ul>
          <li><a href="#conditional-probability">Conditional Probability</a></li>
        </ul>
      </li>
      <li><a href="#approx-string-search">Approx String Search</a>
        <ul>
          <li><a href="#spelling-correction">Spelling Correction</a></li>
        </ul>
      </li>
      <li><a href="#information-retrieval">Information Retrieval</a>
        <ul>
          <li><a href="#searching">Searching</a></li>
          <li><a href="#approaches-to-retrieval">Approaches to retrieval</a></li>
          <li><a href="#boolean-querying">Boolean querying</a></li>
          <li><a href="#ranking">Ranking</a></li>
          <li><a href="#cosine-with-tfidf-weighting-model">Cosine with TFIDF weighting model</a></li>
        </ul>
      </li>
      <li><a href="#web-search">Web Search</a>
        <ul>
          <li><a href="#add-on-technologies">Add-on technologies</a></li>
          <li><a href="#web-crawler">Web crawler</a></li>
          <li><a href="#inverted-list">Inverted list</a></li>
          <li><a href="#phrase-queries">Phrase queries</a></li>
          <li><a href="#pagerank-overview">Pagerank overview</a></li>
        </ul>
      </li>
      <li><a href="#machine-learning">Machine Learning</a>
        <ul>
          <li><a href="#supervised">Supervised</a></li>
          <li><a href="#unsupervised">Unsupervised</a></li>
        </ul>
      </li>
      <li><a href="#testing-strategy">Testing strategy</a>
        <ul>
          <li><a href="#bias-and-variance">Bias and variance</a></li>
        </ul>
      </li>
      <li><a href="#evaluation-metrics">Evaluation Metrics</a>
        <ul>
          <li><a href="#information-retrieval">Information retrieval</a></li>
          <li><a href="#classification">Classification</a></li>
          <li><a href="#clustering">Clustering</a></li>
        </ul>
      </li>
      <li><a href="#recommendation-system">Recommendation System</a>
        <ul>
          <li><a href="#content-bases">Content bases</a></li>
          <li><a href="#collaborative-filtering">Collaborative Filtering</a></li>
        </ul>
      </li>
      <li><a href="#rule-mining">Rule mining</a>
        <ul>
          <li><a href="#approaches">Approaches</a>
            <ul>
              <li><a href="#brute-force-prohibitive">Brute-force (prohibitive)</a></li>
              <li><a href="#two-step-approach">Two-step approach</a></li>
            </ul>
          </li>
          <li><a href="#techniques">Techniques</a>
            <ul>
              <li><a href="#apriori">Apriori</a></li>
              <li><a href="#generate-hash-tree">Generate Hash Tree</a></li>
            </ul>
          </li>
          <li><a href="#further-issues">Further Issues</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#tao-lu">Tao Lu</a>
    <ul>
      <li><a href="#calculating-document-ranking-for-query">Calculating document ranking for query</a></li>
      <li><a href="#dicision-tree-select-splitting-attr">Dicision Tree Select Splitting Attr</a>
        <ul>
          <li><a href="#using-ig">Using IG</a></li>
          <li><a href="#using-gain-ratio">Using Gain Ratio</a></li>
          <li><a href="#using-gini-splite">Using GINI-Splite</a></li>
        </ul>
      </li>
      <li><a href="#accumulator">Accumulator</a>
        <ul>
          <li><a href="#acc-limiting-approach">Acc limiting approach</a></li>
          <li><a href="#acc-threshold-approach">Acc threshold approach</a></li>
        </ul>
      </li>
      <li><a href="#calculate-evaluation-matrics-for-classifier">Calculate Evaluation Matrics for <strong>Classifier</strong></a></li>
      <li><a href="#calculate-user-baseditem-based-recommendation-system">Calculate user-based/item-based recommendation system</a></li>
    </ul>
  </li>
</ul>

<h2 id="basic-concepts">Basic Concepts</h2>

<ul>
  <li><strong>Data</strong> Measurements</li>
  <li><strong>Information</strong> Processed data, patterns that are satisfied for given data</li>
  <li><strong>Knowledge</strong> Information interpretted with respect to a user’s context to extend human understanding in a given area.</li>
  <li><strong>Concrete Tasks</strong> Mechanically processing data to an unambiguous solution; Limited contribution to human understanding</li>
  <li><strong>Knowledge tasks</strong> Data is unreliable or the outcome is ill-defined; Computers mediate between user and the data, where context is critical; Enhance human understanding</li>
</ul>

<h2 id="document-representation">Document representation</h2>
<ul>
  <li><strong>Structured data</strong> Conforms to a schema</li>
  <li><strong>Semi-Structured data</strong> Conforms in part to a schema</li>
</ul>

<h2 id="string-processing">String processing</h2>

<h3 id="regular-expression">Regular expression</h3>
<ul>
  <li>* : Zero or more</li>
  <li>\? : Zero or one</li>
  <li>+ : One or more</li>
</ul>

<p>They are greedy</p>
<ul>
  <li>{m,n} : Between m and n inclusively</li>
  <li>[0-9] = \d</li>
  <li>[a-zA-Z0-9] = \w</li>
  <li>[\ \t\r\n\f] = \s</li>
  <li>[^0-9] = \D</li>
  <li>[^a-zA-Z0-9] = \W</li>
  <li>[^\ \t\r\n\f] = \S</li>
</ul>

<p>Placing a pattern in parentheses leads to the match being stored as a var</p>
<ul>
  <li>\n : nth var</li>
</ul>

<h2 id="similarity-of-text-documents">Similarity (of text documents)</h2>

<h3 id="terminologies">Terminologies</h3>

<ul>
  <li><strong>$f_d$</strong> number of terms in document d</li>
  <li><strong>$f_{d,t}$</strong> Freq of term t in document d (TF)</li>
  <li><strong>$f_{ave}$</strong> The average number of terms contained in a document</li>
  <li><strong>$N$</strong> Number of documents</li>
  <li><strong>$f_t$</strong> Number of documents that contains t</li>
  <li><strong>$F-t</strong> Total number of occurrence of t across all documents</li>
  <li><strong>$n$</strong> the number of indexed terms in the collection</li>
</ul>

<h3 id="tf-idf">TF-IDF</h3>
<ul>
  <li>Terms that occur frequently in a given document have high utility</li>
</ul>

<p><script type="math/tex">w_{d,t} \propto f_{d,t}</script></p>
<ul>
  <li>Terms that occur in a wide varity of docs have low utility</li>
</ul>

<script type="math/tex; mode=display">w_{t} \propto \frac{1}{f_t}</script>

<p>Weight up these two vectors</p>

<script type="math/tex; mode=display">w_{d,t} = f_{d,t} \times log\frac{N}{f_t}</script>

<h3 id="jaccard-similarity">Jaccard Similarity</h3>

<script type="math/tex; mode=display">\frac{|A \cap B |}{|A \cup B|}</script>

<h3 id="dice-similarity">Dice Similarity</h3>

<script type="math/tex; mode=display">\frac{2|A\cap B|}{|A| + |B|}</script>

<h3 id="cosine-distance">Cosine Distance</h3>

<script type="math/tex; mode=display">sim(A,B) = \frac{\vec{a}\cdot\vec{b}}{|\vec{a}||\vec{b}|}</script>

<h3 id="relative-entropy-kullback-leibler-deivergence">Relative entropy (Kullback-Leibler deivergence)</h3>

<p>A measure of difference between to probability distributions</p>

<script type="math/tex; mode=display">D(X||Y) = \sum_i x_i log2\frac{x_i}{y_i}</script>

<h3 id="skew-divergence">Skew divergence</h3>

<script type="math/tex; mode=display">s_\alpha(X,Y) = D(X||\alpha Y + (1-\alpha) X)</script>

<h3 id="jensen-shannon-divergence">Jensen-Shannon divergence</h3>

<script type="math/tex; mode=display">JSD(X||Y) = \frac{1}{2}D(X||m) + \frac{1}{2}(Y||m)</script>

<p>Where $m = \frac{X+Y}{2}$</p>

<h2 id="probability">Probability</h2>

<h3 id="conditional-probability">Conditional Probability</h3>
<ul>
  <li>
    <p>Sum rule
<script type="math/tex">P(A) = \sum_BP(A\cap B )</script></p>
  </li>
  <li>
    <p>Multiplication rule
<script type="math/tex">P(A\cap B) = P(A|B)P(B)</script></p>
  </li>
  <li>
    <p>Bayes rule
<script type="math/tex">P(B|A) = \frac{P(A\cap B)}{P(A)} = \frac{P(A|B)P(B)}{P(A)}</script></p>
  </li>
</ul>

<h2 id="approx-string-search">Approx String Search</h2>

<h3 id="spelling-correction">Spelling Correction</h3>

<ul>
  <li>Neighbourhood search
    <ul>
      <li>Generate all variants of w that utilise at most k changes (Insertions/Deletions/Replacements)</li>
      <li>Check whether generated variants exist in dictionary</li>
      <li>All results found are returned</li>
      <li>
        <table>
          <tbody>
            <tr>
              <td>For k edits, $O(\Sigma^k \cdot</td>
              <td>w</td>
              <td>^k )$ neighbours where $\Sigma$ is alphabet size</td>
            </tr>
          </tbody>
        </table>
      </li>
    </ul>
  </li>
  <li>Global Edit distance
    <ul>
      <li>Transform the string of interest into each dictionary entry, using insert, delete, replace and match</li>
      <li>How to calculate: First init the table, left top corner = 0, going along “from” cause addition of “Delete”, going along “to” direction cause addition of “Insert”. Go through each cell one-by-one, take max of inserting from last row, deleting from last column or match/replace from left-up cell. Last cell is the result.</li>
    </ul>
  </li>
  <li>Local edit distance: This is like global edit distance, but we are searching the best substring match.
    <ul>
      <li>how to calculate: Init table, but this time first row and col as 0. Go through just like before, but max() take 0 as additional argument as well. Final result is the greatest value in the table.</li>
    </ul>
  </li>
  <li>
    <p>N-Gram Distance
 <script type="math/tex">|G_n(s)| + |G_n(t)| - 2\times |G_n(s) \cap G_n(t)|</script></p>
  </li>
  <li>Soundex
    <ul>
      <li>Except for init char, translate string chars according to the table</li>
      <li>remove duplicates</li>
      <li>remove 0s</li>
      <li>truncate to four symbols</li>
    </ul>
  </li>
</ul>

<h2 id="information-retrieval">Information Retrieval</h2>

<p>IR is the subfield of computer science that deals with storage and retrieval of documents</p>

<p>Documents are not always text. They can be defined as messages: an object that conveys information from one person to another</p>

<ul>
  <li>Stored documents are real-world objects that have been created for individual reasons without consistent format, wording, language, length</li>
  <li>The retrieval system is concerned with the document as originally created not with a formal representation of the document</li>
  <li>Users may not agree on the value of a particular document, even in relation to the same query</li>
  <li>Documents are rich and ambiguous</li>
  <li>Text in some kind of collection has structured attrs, but these are only occasionally useful for searching</li>
</ul>

<h3 id="searching">Searching</h3>

<p>Categories of searching:</p>
<ul>
  <li>Informational</li>
  <li>Factoid</li>
  <li>Topic tracking</li>
  <li>Navigational</li>
  <li>Transactional</li>
  <li>Geospatial</li>
</ul>

<h3 id="approaches-to-retrieval">Approaches to retrieval</h3>
<p>Consider the criteria that a human might use to judge whether a document should be returned in response to a query.</p>
<ul>
  <li>Try and guess what the query might be inspired by, and what kind of info or doc is being sought</li>
  <li>Consider current news and events, or their own experience with query terms</li>
  <li>Approach the task of looking through the docs with expectations of what a match is that is based on much more than the terms</li>
  <li>Be ready to consider a doc even if the terminology is completely different</li>
</ul>

<h3 id="boolean-querying">Boolean querying</h3>
<p>Documents match if they contain the terms and don’t contain the NOT terms. There is no ordering, only yes/no (Start with least frequent terms to reduce cost)</p>

<h3 id="ranking">Ranking</h3>
<p>By looking for evidence in the document that it is on the same topic as the query</p>

<ul>
  <li>Choose docs with words in common with query</li>
  <li>Choose docs with the query terms oin title</li>
  <li>Created recently</li>
  <li>Translate between languages</li>
  <li>Choose authoritative, reliable documents</li>
</ul>

<h3 id="cosine-with-tfidf-weighting-model">Cosine with TFIDF weighting model</h3>
<p>This is nothing more than calculating the cosine distance between query the documents. The term $w_{d,t}$ and $w_{q,t}$ are just vector representation of document and query using the key terms. Sometimes they are just TF and IDF. In most cases, they are given in questions.</p>

<script type="math/tex; mode=display">S(q,d) = \frac{\sum_tw_{d,t}\times w_{q,t}}{|q||d|}</script>

<p>How to calculate?</p>
<ul>
  <li>Remember the fucking terminology:</li>
  <li>N is the fucking number of documents</li>
  <li>$f_t$ is the number of fucking documents which contains the fucking term</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>$</td>
          <td>q</td>
          <td>$ and $</td>
          <td>d</td>
          <td>$ are the length of the god damn vector representation of the fucking query and document using given $w_{d,t} and w_{q,t}$</td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<h2 id="web-search">Web Search</h2>
<p>Main technological components:</p>
<ul>
  <li><strong>Crawling</strong></li>
  <li><strong>Parsing</strong> Translate data into canonical form</li>
  <li><strong>Indexing</strong> Data structure build to allow search to take place efficiently</li>
  <li><strong>Querying</strong> The data structures must be processed in response to queries</li>
</ul>

<h3 id="add-on-technologies">Add-on technologies</h3>
<ul>
  <li>Snippet generation</li>
  <li>As-you-type querying</li>
  <li>Query correction</li>
  <li>Answer consolidation</li>
  <li>Info boxes</li>
  <li>Tokenization: reduce a webpage or a query to a sequence of tokens</li>
  <li>Stemming: Stripping away affixes. implemented as cascaded set of rewrite rules.</li>
  <li>
    <p>Zoning: Web documents can usually be segmented into discrete zones such as title, anchor text, headings and so on. Calculating the weight of each zone</p>
  </li>
  <li>Indexing:
    <ul>
      <li><strong>Search structure</strong> For each word t, the search structure contains a pointer to the start of the corresponding inverted list, and a count $f_t$ of the documents containning t</li>
      <li><strong>Inverted lists</strong> For each word t, the inverted list contains the identifier d of documents containing t as ordinal numbers, and the associated freq $f_{d,t}$ of t in d.</li>
    </ul>
  </li>
  <li>Inverted list allows for fast querying because
    <ul>
      <li>The terms in the query correspind to the search structure</li>
      <li>The index only indicates documents where the term is present</li>
    </ul>
  </li>
  <li>
    <p>Ranked querying</p>
  </li>
  <li>
    <p>Link analysis</p>

    <p>A string piece of evidence for a page’s importance is given by links, how many pages have links to this page</p>
  </li>
  <li>
    <p>Pagerank</p>

    <p>Use random walks to calculate the $\pi(d)$ value for page, with assumption that each page has same probability of being the start point and probabilities of visiting outgoing links are equal.</p>
  </li>
</ul>

<h3 id="web-crawler">Web crawler</h3>
<ul>
  <li>Challenges:
    <ul>
      <li>There is no central index of URLs of interest</li>
      <li>Some website return the same content as a new url at each visit</li>
      <li>Some pages never return status done on access</li>
      <li>Some websites are not intended to be crawled</li>
      <li>Much web content is generated on-the-fly from databases, which can be costly for content provider, so excessive numbers of visits to a site are unwelcome</li>
      <li>Some contents has a short lifespan</li>
      <li>Some regions and content providers have low bandwidth</li>
    </ul>
  </li>
</ul>

<h3 id="inverted-list">Inverted list</h3>

<ul>
  <li>Search structure
    <ul>
      <li>Pointer to the start of inverted list for each term</li>
      <li>A count of docs that contains t $f_t$</li>
    </ul>
  </li>
  <li>Inverted list
    <ul>
      <li>d that contains t</li>
      <li>Freq of t in d(We could store $w_{d,t}$ or $\frac{w_{d,t}}{W_{d\cdot}}$)</li>
    </ul>
  </li>
  <li>Boolean Querying
    <ul>
      <li>Fetch inverted lists</li>
      <li>Union for OR</li>
      <li>Intersection for AND</li>
      <li>Complement for NOT</li>
    </ul>
  </li>
</ul>

<h3 id="phrase-queries">Phrase queries</h3>
<p>How to find the pages in which the words occur as a phrase</p>

<p>Strategies:</p>
<ul>
  <li>Process as bag of words, then post-process (Can be slow, start with most infrequent terms to speed up)</li>
  <li>Add position to index entries</li>
  <li>Use some form of phrase index of word-pair index so that they can be directly indetified without using inverted index</li>
</ul>

<h3 id="pagerank-overview">Pagerank overview</h3>

<h2 id="machine-learning">Machine Learning</h2>

<h3 id="supervised">Supervised</h3>
<ul>
  <li><strong>Classification</strong>
    <ul>
      <li><strong>Decision Tree</strong>
        <ul>
          <li>Best split (Check out Tao Lu)</li>
          <li>
            <p>Parameters: Total number of nodes, depth, minimum number of data points for a split</p>
          </li>
          <li>How to set params? Cross-validation</li>
          <li>Continuous attributes: Discretization at the beginning or find ranges by equal interval bucketing</li>
        </ul>
      </li>
      <li><strong>Random forest</strong>: train multiple trees on random subset of samples, decision via majority voting (Can be subset of record, subset of attr)
        <ul>
          <li>Num of trees</li>
          <li>Use bagging to come up with different training dataset for each tree</li>
          <li>When building our tree, at each node, we only consider a random sample of attributes.</li>
        </ul>
      </li>
      <li><strong>SVM</strong>
        <ul>
          <li>Find hyperplane that maximises the margin (find w and b)</li>
          <li>Lagrange multiplier applied to get dual problem: Solve $\alpha$</li>
          <li>$x_i$ with non-zero $\alpha_i$ will be support vectors</li>
          <li>Softmargin introduces parameters to tradeoff the relative importance of maximizing the margin and fitting the data</li>
          <li>Use kernel function for non-linear SVM</li>
          <li>One-vs-all multi-class strategy: Build M classifiers for M classes, choose class with largest margin for test data</li>
          <li>One-vs-one: One classifier per pair, choose class selected by most classifiers</li>
        </ul>

        <p>How to derive</p>

        <p>Write down the margin by calculating the distance between 2 aprallel lines
  <script type="math/tex">(b-1) + x_0w_0 + x_1w_1 + ... = 0</script>
   and
  <script type="math/tex">(b+1) + x_0w_0 + x_1w_1 + ... = 0</script></p>

        <p>Distance between parallel lines is given by 
  <script type="math/tex">d = \frac{ax_1+by + c}{\sqrt{a^2 + b^2}}</script> 
  where $x_1$ and $y_1$ are points on one line and $ax+by+c=0$ is another line.</p>

        <p>Then maximize the margin using constraint optimization with Lagrange multiplier $\alpha$. Write all the parameter in form of $\alpha$, then solve the dual problem, which is dead easy.</p>

        <p>For soft margin, trade off the width of margin and how many points have to be moded around. The margin can be smaller than 1 for a point $x_i$ by setting $\xi_i&gt;0$ but pay penalty of $C\xi_i$ in the minimization objective, i.e., minimize 
  <script type="math/tex">w^Tw + C\sum_i\xi_i</script>
  st
  <script type="math/tex">y_i(w^Tx_i + b) >= 1-\xi_i</script></p>
      </li>
      <li>
        <p><strong>KNN</strong></p>

        <p>Select K nearest neighbours and look at their label</p>
      </li>
      <li>
        <p><strong>Naive Bayes</strong>
   <script type="math/tex">c = argmax_cP(c)\prod P(x|c)</script></p>
      </li>
      <li><strong>NN</strong>
        <ul>
          <li>Define E</li>
          <li>Forward pass: calculate hidden $H_i$ and output $O_j$ values</li>
          <li>Calculate error for each layer</li>
          <li>Update weight using gradient descent</li>
          <li>Until converge</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="unsupervised">Unsupervised</h3>
<ul>
  <li>Association</li>
  <li>Clustering
    <ul>
      <li><strong>K-means</strong>
        <ul>
          <li>Init clusters</li>
          <li>Assign cluster</li>
          <li>Recalculate cluster center</li>
          <li>Reassign cluster …</li>
          <li>+ Efficient</li>
          <li>+ Cane be extended to hierarchical clustering</li>
          <li>- Local minimum</li>
          <li>- Need k in advance</li>
          <li>- Unable to handle un-convex</li>
          <li>- Ill defined “mean”</li>
          <li>- Data contains outliers</li>
        </ul>
      </li>
      <li><strong>Buttom-up</strong>
        <ul>
          <li>Start with single-instance clusters</li>
          <li>Iteratively merge closest two</li>
        </ul>
      </li>
      <li><strong>Top-down</strong>
        <ul>
          <li>Start with one</li>
          <li>Find two partitioning clusters</li>
          <li>Proceed recursively on each subset</li>
        </ul>
      </li>
      <li><strong>Cluster dist measurement</strong>
        <ul>
          <li>MIN (single link) use two closest points in clusters</li>
          <li>MAX (complete link) Use two farthest points in clusters</li>
          <li>Group average Use average dist between all points</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Reinforcement learning</li>
  <li>Recommender system</li>
  <li>Anomaly dection</li>
</ul>

<h2 id="testing-strategy">Testing strategy</h2>

<h3 id="bias-and-variance">Bias and variance</h3>
<ul>
  <li><strong>(Training) bias</strong> is the average distance between expected value and estimated value</li>
  <li>
    <p><strong>(Testing) variance</strong> is the standard deviation between the estimated value and the average estimated value(Inconsistency of decision)</p>
  </li>
  <li><strong>Holdout</strong> Fixed training and testing set
    <ul>
      <li>+ Simple</li>
      <li>+ High reproducibility</li>
      <li>- Tradeoff: more training or testong</li>
      <li>- Representativeness of the training the test data</li>
    </ul>
  </li>
  <li><strong>Random subsampling</strong> Multiple iterations, randomly selecting training and test data
    <ul>
      <li>+ Reduce bias and variance</li>
      <li>- Reproducability</li>
    </ul>
  </li>
  <li><strong>Leave one out</strong> Train N times, 1 test instance each time. Measure average performance
    <ul>
      <li>+  No sampling bias</li>
      <li>+ Higher accuracy</li>
      <li>- Expensive</li>
    </ul>
  </li>
  <li><strong>M-fold cross validation</strong>
    <ul>
      <li>+ Only train M times</li>
      <li>+ Can measure stability of the system</li>
      <li>- Sampling bias</li>
      <li>- Results varies unless partition identically</li>
      <li>- Slightly low acc</li>
      <li>- Not suitable for small dataset</li>
    </ul>
  </li>
  <li>
    <p><strong>Regression</strong></p>
  </li>
  <li>
    <p><strong>Feature Selection</strong></p>

    <p>To choose a subset of attributes that give best performance on the development data (Slow)</p>

    <p>Greedy approach:</p>
    <ul>
      <li>Train and eval on each single attr</li>
      <li>Choose best attr</li>
      <li>Repeat
        <ul>
          <li>Train and evel model on best attrs, plus each remaining single attr</li>
          <li>Choose best attr out of remaining set</li>
        </ul>
      </li>
      <li>Until performance stops increasing</li>
    </ul>

    <p>Ablation approach:</p>
    <ul>
      <li>Start with all attributes</li>
      <li>Remove one, train and eval</li>
      <li>Until divergence
        <ul>
          <li>From remaining attr, remove each attr, train and eval</li>
          <li>Remove the attr that cause least performance degredation</li>
        </ul>
      </li>
      <li>Termination condition: Performance starts to degrade by more than $\epsilon$</li>
    </ul>

    <p>Mutual Information Approach</p>

    <p><script type="math/tex">P(A,C) = P(A)P(C)</script>
  if attr independent from class. Which means 
  <script type="math/tex">P(C|A) = P(C)</script></p>

    <p>The equation 
  <script type="math/tex">\frac{P(A,C)}{P(A)P(C)} = 1</script></p>

    <ul>
      <li>If $LHS \approx 1$, almost random chance</li>
      <li>If $LHS »1$, they occur together much more often than randomly</li>
      <li>If $LHS  « 1$, negatively correlated</li>
    </ul>

    <p>Therefore 
  <script type="math/tex">PMI(A,C) = \log_2\frac{P(A,C)}{P(A)P(C)}</script></p>

    <p>Greatest PMI indicates best attribute valie A for class C. Taking into consideration of all the possible classes and attribute values</p>

    <script type="math/tex; mode=display">MI(A,C) = \sum_{i\in{a, \bar a}}\sum_{j\in{c, \bar c}}P(i,j)PMI(i,j)</script>
  </li>
</ul>

<h2 id="evaluation-metrics">Evaluation Metrics</h2>

<ul>
  <li><strong>Generalise</strong> when it learns the target function well, rather than specifics of the training set.</li>
  <li><strong>Overfitting</strong> when the classifier fails to generalise(describe training set very well, but does not describe the test data well)</li>
</ul>

<h3 id="information-retrieval-1">Information retrieval</h3>
<ul>
  <li><strong>Precision</strong> Fraction of correct responses among attempted responses</li>
  <li><strong>Recall</strong> Proportion of words with a correct response</li>
  <li><strong>Precision at k(P@k)</strong> Fraction of number of returned relevant results in top k</li>
  <li>
    <p><strong>Average Precision</strong> 
  A single number that characterizes the performance instead of comparing curves.</p>

    <p>This is basically integrating precision pver recall from 0 to 1. Descretize the calculation to single samples to get the following form:</p>

    <p><script type="math/tex">AP=\frac{1}{R}\sum_{k|d(k)isrelevent}P@k</script>
Where R is total num of relevant docs for query</p>
  </li>
</ul>

<h3 id="classification">Classification</h3>
<ul>
  <li>
    <p><strong>Accuracy</strong> Proportion of correctly classified instance among all instances
<script type="math/tex">ACC=\frac{TP + TN}{TP + FP + TN + FN}</script></p>
  </li>
  <li><strong>Error Rate</strong> 1-ACC</li>
  <li><strong>Error Rate Reduction</strong>
<script type="math/tex">ERR = \frac{ER_0-ER}{ER_0}</script>
where $ER_0$ is baseline ER</li>
  <li><strong>Precision</strong> Proportion of correct positive predictions
<script type="math/tex">Precision=\frac{TP}{TP + FP}</script></li>
  <li><strong>Recall</strong> Proportion of correctly predicted positive instances among all actual positive instances</li>
  <li><strong>Specificity</strong> Proportion of correctly predicted negative instances among all actual negative instances.</li>
  <li><strong>F-score</strong> Gives an overall performance
<script type="math/tex">Fscore = (1+\beta^2)\frac{PR}{R + \beta^2P}</script></li>
  <li><strong>ROC(Receiver Operating Characteristics)</strong> Best prediction yields upper left corner.</li>
  <li><strong>AUC(Area Under the Curve)</strong> Probability that a classifier will rank a randomly chosen instance higher than a randomly chosen negative one.</li>
  <li><strong>Micro-averaging</strong> Summing up numerator and denominator for each class</li>
  <li><strong>Macro-averaging</strong> Summing up the results then divide by number of classes</li>
</ul>

<h3 id="clustering">Clustering</h3>
<ul>
  <li><strong>Unsupervised</strong> Check the cohesion and separation of the clusters.</li>
  <li><strong>Supervised</strong> Compare the cluster structure with external structure. For example entropy in a cluster is the measurement of the purity of instances.</li>
  <li><strong>Relative</strong> Compares different clusterings with either unsupervised or supervised evaluation strategy.</li>
  <li><strong>SSE</strong> Sum of squared error. Sum of the square of distance of each instance to the center of cluster for all the clusters.</li>
</ul>

<h2 id="recommendation-system">Recommendation System</h2>

<h3 id="content-bases">Content bases</h3>
<ul>
  <li>Can recomment new items</li>
  <li>Feature extraction can be difficult</li>
</ul>

<h3 id="collaborative-filtering">Collaborative Filtering</h3>
<ul>
  <li><strong>User-based</strong> Similar users have similar ratings on the same item.
    <ul>
      <li>Identify set of items rated by the target user</li>
      <li>Identify other users rated at least 1 items in this set</li>
      <li>Compute how similar each neighbor is to the target user</li>
      <li>Select k most similar neighbors</li>
      <li>Predict ratings for the target user’s unrated items</li>
      <li>Recommend to the user top N products based on predicted rating</li>
    </ul>

    <p>User Similarity (Pearson correlation)
  <script type="math/tex">r(X,Y) = \frac{\sum_{i}(X_i-\bar X)(Y_i -\bar Y)}{\sqrt{\sum_i(X_i-\bar X)^2}\sqrt{\sum_i(Y_i-\bar Y)^2}}</script></p>

    <p>Predicted ratings</p>

    <script type="math/tex; mode=display">r^*_{uj} = \mu_u + \frac{\sum_{v\in P_u(j)}Sim(u,v)\cdot(r_{vj} - \mu_v) }{\sum_{v\in P_u(j)}|Sim(u,v)|}</script>
  </li>
  <li><strong>Item-based</strong> Similar items are rated in a similar way by the same user
    <ul>
      <li>Identify set of users who rated the target item i</li>
      <li>Identify which other items were rated</li>
      <li>Compute similarity between each neighbour and target item</li>
      <li>Select k most similar neighbours</li>
      <li>Predict ratings for the target item</li>
    </ul>
  </li>
  <li><strong>Challenges</strong>
    <ul>
      <li>Few data each user</li>
      <li>Too many items to choose from</li>
      <li>New user no data</li>
      <li>Few recommendation to propose</li>
      <li>Large dataset</li>
    </ul>
  </li>
</ul>

<h2 id="rule-mining">Rule mining</h2>

<ul>
  <li>
    <p><strong>Itemset</strong> is a collection of one or more items, like {A,B,C}.</p>
  </li>
  <li><strong>k-itemset</strong> contains k items.</li>
  <li><strong>Support count</strong> ($\sigma$) is freq of occurrance of itemset</li>
  <li><strong>Support</strong> is fraction of transactions that contain an itemset</li>
  <li><strong>Frequest Itemset</strong> is an itemset whose support is greater than or equal to a minsup threshold</li>
  <li>Association Rule
An implication expression of form $A \rightarrow B$ where A and B are itemsets
    <ul>
      <li>Support: Fraction of transactions that contain both A and B
<script type="math/tex">s(A\rightarrow B) = \frac{\sigma(A\cup B)}{\sigma(*)}</script></li>
      <li>
        <p>Confidence: Measure how often item in A appear in transactions that contain B
<script type="math/tex">c(A\rightarrow B) = \frac{\sigma(A\cup B)}{\sigma(A)}</script></p>
      </li>
      <li>Useful: High quality, actionable information</li>
      <li>Trivial: Already known to anyone familiar with the context</li>
      <li>Inexplicable: This which have no apparent explanation</li>
    </ul>
  </li>
</ul>

<h3 id="approaches">Approaches</h3>

<h4 id="brute-force-prohibitive">Brute-force (prohibitive)</h4>
<ul>
  <li>List all possible rules</li>
  <li>Compute support and confidence</li>
  <li>Prune rules that fail the minsup and minconf</li>
</ul>

<h4 id="two-step-approach">Two-step approach</h4>
<ul>
  <li>Frequent itemset generation
    <ul>
      <li>Reduce candidate (Apriori)</li>
      <li>Reduce transaction</li>
      <li>Reduce number of comparisons</li>
    </ul>
  </li>
  <li>Rule generation (Generate Hash Tree)</li>
</ul>

<h3 id="techniques">Techniques</h3>
<h4 id="apriori">Apriori</h4>
<p>Subset always have higher support</p>

<ul>
  <li>Let k=1</li>
  <li>Generate frequent itemsets of length 1</li>
  <li>Repeat until no new frequent itemsets are identified
    <ul>
      <li>Prune candidate itemsets containing subset of length k that are infreq</li>
      <li>Count support of each candidate by scanning database</li>
      <li>Eliminate infreq candidates</li>
      <li>Generate length k+1 candidate itemsets from length k itemsets</li>
    </ul>
  </li>
</ul>

<h4 id="generate-hash-tree">Generate Hash Tree</h4>

<p>Accelerates counting support count for candidates</p>

<ul>
  <li>Build up an hashtree by hashing items in candidate itemsets</li>
  <li>Given a transaction, start from 1th layer, recursively:
    <ul>
      <li>At $k^{th}$ layer, hash $k^{th}, k+1^{th}…k+n^{th}$ items at current node, until the leaf of the tree. Then Compare the transaction with candidates at leaf node.</li>
    </ul>
  </li>
</ul>

<h3 id="further-issues">Further Issues</h3>
<ul>
  <li>Correlation vs causation
    <ul>
      <li>May be wrong direction</li>
      <li>Causation with unseen intermediary</li>
      <li>Unseen cause of both events</li>
      <li>Coincidental</li>
    </ul>
  </li>
  <li>Data mining
    <ul>
      <li>Valid: Data has support for the pattern</li>
      <li>Non-trivial: The pattern isn’t self-evident from the data</li>
      <li>Previously unknown</li>
    </ul>
  </li>
</ul>

<h1 id="tao-lu">Tao Lu</h1>

<h2 id="calculating-document-ranking-for-query">Calculating document ranking for query</h2>

<ul>
  <li>Write vector of documents using given tf formula</li>
  <li>Write vector of queries using given idf formula. Pay attention that when the freq of term in query is zero, the value should probabily be 0</li>
  <li>Calculate the similarity</li>
</ul>

<h2 id="dicision-tree-select-splitting-attr">Dicision Tree Select Splitting Attr</h2>

<h3 id="using-ig">Using IG</h3>
<ul>
  <li>Calculate the root entropy using class distribution $P(C)$</li>
  <li>For each attribute A, calculate:
    <ul>
      <li>Entropy of $P(C)$ for the data subset separated by different A values.</li>
      <li>Sum up the entropies with weight (attr A value count distribution) as H(A)</li>
      <li>Calculate IG(A) by subtracting from H(R)</li>
    </ul>
  </li>
</ul>

<h3 id="using-gain-ratio">Using Gain Ratio</h3>
<ul>
  <li>Gain ratio is obtained by dividing IG using SI</li>
  <li>SI (Split information) is the entropy of attr count distribution</li>
</ul>

<h3 id="using-gini-splite">Using GINI-Splite</h3>

<ul>
  <li>Calculate GINI(R)</li>
  <li>Calculate weighted sum of GINI of different values of the attr</li>
  <li>Substract to get GS value of the attr</li>
</ul>

<p>GINI is the 1- sum of probability square</p>

<h2 id="accumulator">Accumulator</h2>
<ul>
  <li>Init accumulator for each doc</li>
  <li>For each query term t
    <ul>
      <li>Add weight product $w_{q,t}w_{d,t}$ to corresponding accumulator for doc in inverted list</li>
    </ul>
  </li>
  <li>Take max</li>
</ul>

<h3 id="acc-limiting-approach">Acc limiting approach</h3>
<p>Limiting the size of accumulators, if hit the limit, stop creating accumulators. (order query terms by $w_{q,t}$)</p>

<h3 id="acc-threshold-approach">Acc threshold approach</h3>
<p>If inner prod is smaller than threshold, do not create accumulator.(order query terms by $w_{q,t}$)</p>

<h2 id="calculate-evaluation-matrics-for-classifier">Calculate Evaluation Matrics for <strong>Classifier</strong></h2>
<ul>
  <li>Look at the confusion matrix, the diagonal elements are correctly classified item counts.</li>
  <li>Accuracy is calculated once per classification, summing up diagonal divided by total item count</li>
  <li>Precision and Recall are calculated once per class. The sum along actual except the diagonal element of a class is FN(because they are incorrect, and negative). Sum along classified except diagonal ones is FP because they are incorrect, and should be positive</li>
  <li>Then use the formula to calculate the god damn values</li>
</ul>

<h2 id="calculate-user-baseditem-based-recommendation-system">Calculate user-based/item-based recommendation system</h2>
<p>Fuck it. Jeremy, if you want me to lose marks, exam this, I won’t remember any single character of the formula. I think remembering this formula is the only motherfucking thing that takes time to do in this subject. I paid my tuition fee to learn something, not this kind of easy shit :).</p>


      <footer class="site-footer">
        
          <span class="site-footer-owner"><a href="http://github.com/wylswz/wylswz.github.io">wylswz.github.io</a> is maintained by <a href="http://github.com/wylswz">wylswz</a>.</span>
        
        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</span>
      </footer>
    </main>
  </body>
</html>
<script>
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    
    processEscapes: true
  }
});
</script>
