<!DOCTYPE html>
<html lang="en-US">

    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript">
          MathJax.Hub.Config({
          tex2jax: {
              inlineMath: [ ['\$','\$'], ["\\(","\\)"] ],
          },

      });
    </script>

 
  <head>

    

      <script async src="https://www.googletagmanager.com/gtag/js?id=UA-141212522-1"></script>
      <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'UA-141212522-1');
      </script>

    
    <meta charset="UTF-8">

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Face Recognition Development Notes | IT Taolu</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="Face Recognition Development Notes" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Read best Taolus of IT industry" />
<meta property="og:description" content="Read best Taolus of IT industry" />
<link rel="canonical" href="http://localhost:4000/FaceDetection/FaceDetection.html" />
<meta property="og:url" content="http://localhost:4000/FaceDetection/FaceDetection.html" />
<meta property="og:site_name" content="IT Taolu" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Face Recognition Development Notes" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"Read best Taolus of IT industry","headline":"Face Recognition Development Notes","url":"http://localhost:4000/FaceDetection/FaceDetection.html"}</script>
<!-- End Jekyll SEO tag -->

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <link rel="stylesheet" href="/assets/css/style.css?v=4cab4a74ddc0c244cdb3a9379be74bd111eeaa7b">
  </head>
  <body>
    <header class="page-header" role="banner">
      <h1 class="project-name">IT Taolu</h1>
      <h2 class="project-tagline">Read best Taolus of IT industry</h2>
      
        <a href="https://github.com/wylswz/wylswz.github.io" class="btn">View on GayHub</a>
      
      
    </header>

    <main id="content" class="main-content" role="main">
      <h1 id="face-recognition-development-notes">Face Recognition Development Notes</h1>

<h2 id="tensorflow">Tensorflow</h2>

<h3 id="graph">Graph</h3>
<p>Tensorflow is a graph-based computation framework. Using graph provides better readibality, parallelism and optimization of pre-compiling.</p>

<p>In declarative programming, the outputs of functions are independent of external states, instead, they only depend on the inputs, which eliminates most troubles caused by the side effect. In tensorflow, each function or operation is a single node in the graph.</p>

<p>The Graph characteristic of tensorflow also provide the capability of pre-compiling. Instead of executing the codes line by line like interpreted language (Python say for example), a graph is generated by pre-compiling the code. Optimization like parallelize independent logics, removal of useless logics and extracion of shared logics are available, thus make it more efficient.</p>

<h3 id="key-elements-in-tensorflow-graph">Key elements in tensorflow graph</h3>

<h2 id="model-design">Model design</h2>

<p>The primary problem of the project is to find selfie among the photos uploaded by twitter users. This problem can be treat as an object detection problem. However, the challenge part is that we can only refer to user’s avatar as how does the user look like. Therefore, it is impossible to train a netowork to classify a photo as sombody. Moreover, we want the model to be generalized so it can detect users that have never be seen before, therfore training the network using user-id as class is infeasible in this case.</p>

<p>Instead of detecting user directly, we introduce two separated steps to accomplish this task. The first one is to detect the object using RCNN-like networks or YOLO(You only look once). The second step is to compare the detected face to the face in user’s avatar, using one-shot algorithm like Siamese Net.</p>

<h3 id="dataset">Dataset</h3>

<p>The <a href="http://www.robots.ox.ac.uk/~vgg/data/vgg_face2/">dataset</a> is a collection of loosely cropped human faces. The filename structure is in the form of “n{id}/{filename}.jpg”. The photos of the same person are placed under the same directory.</p>

<h3 id="rcnn">RCNN</h3>
<p>RCNN is on of the simplest way of object detection. It works as follows:</p>

<ul>
  <li>Pre-process the input image to a certain size (Cropping or wrapping)</li>
  <li>Select some regions using region proposal algorithms based on the features of the image like edges, colors, etc… Typically 1000 to 2000 regions proposed for each image</li>
  <li>For each region, run CNN to extract the feature</li>
  <li>Use SVM to classify the object</li>
  <li>Output class + bounding box</li>
</ul>

<p>Advantage: Very simple</p>

<p>Disadvantage: Repeated CNN make it very slow, high computation waste.</p>

<h3 id="fast-rcnn">Fast RCNN</h3>
<p>Fast RCNN improve the efficiency by following changes.</p>

<ul>
  <li>Running convolution on the image for only once at the beginning</li>
  <li>Proposed regions are mapped to the output of convolution net directly instread of run convnet on original regions.</li>
  <li>Using convolution implementation for fully connected layer.</li>
</ul>

<h3 id="faster-rcnn">Faster RCNN</h3>
<p>Faster RCNN makes it even faster by proposing bounding boxes using pre-trained neural networks.</p>

<h4 id="yolo">YOLO</h4>
<p>YOLO is even faster than faster-RCNN but it has trade accuracy off for speed to make real-time detection in video stream possible.</p>

<p>The main idea of YOLO is dividing the image into grids, say 7 by 7 grids. Each grid cell predicts B bounding boxes and corresponding confidence.</p>

<p>In prediction session, the model infers the class-wise confidence of each grid cell. This indicates how well the object belongs to a class and how well the object is fitted.</p>

<p>The limitation of YOLO is that when there are overlapping between objects or multiple objects appear in the same box, it can hardly operate well.</p>

<h3 id="nn-shit">NN shit</h3>

<h4 id="gradient-descent">Gradient descent</h4>

<p>Gradient descent method are usually applied to perform iterative optimization of linear or non-linear (using jacobian method). The main idea is keeping taking stepin the direction of opposite to the gradient at current point, where we need to calculate the partial derivative of the less function with respect to each variable.</p>

<h4 id="back-propagation">Back propagation</h4>
<p>The main idea of this section comes from <a href="https://en.wikipedia.org/wiki/Backpropagation">wikipedia</a></p>

<p>Back propagation is when we apply gradient descent method to a neural network. In this method, we start from the output layer and move backward, which is why it’s called back propagation. The variable part of the NN is weight, thus we calculate the partial derivative of less function with respect to the weights of incoming edges of current layer. That is,</p>

\[\frac{\partial E}{\partial w_{i,j}}\]

<p>However, this is quite hard to compute directly. Therefore we apply chain rule to the equation above:</p>

\[\frac{\partial E}{\partial w_{i,j}} = \frac{\partial E}{\partial o_j}\frac{\partial o_j}{\partial net_j}\frac{\partial net_j}{\partial w_{i,j}}\]

<p>, where $w_{i,j}$ is the weight of the edge connection neuron $i$ and $j$, $o_j$ is the output of neuron $j$ abd $net_j$ is the sum of neuron $j$, which is the sum of all incoming edges times the output of the neuron tge edge comes from.</p>

<p>From the last term of the equation above, we can find that</p>

\[\frac{\partial net_j}{\partial w_{i,j}} = \frac{1}{\partial w_{i,j}}(\sum_{k=1}^{n}w_{k,j}o_{k}) = \frac{1}{\partial w_{i,j}}w_{i,j}o_{i} = o_i\]

<p>This is because only one term in the sum expression above depends on the weight $w_{i,j}$.</p>

<p>For the second term,</p>

\[\frac{\partial o_j}{\partial net_j} = \frac{1}{\partial net_j} \varphi(net_j) = \varphi(net_j)(1-\varphi(net_j))\]

<p>Where the term $\varphi(…)$ is the activation function of the neuron, this is why we prefer a differentiable activation function for many cases.</p>

<p>The first term partial derivative is given by:</p>

\[\frac{\partial E}{\partial o_j} = \frac{\partial E}{\partial y} = \frac{1}{\partial y} \frac{1}{2}(t-y)^2 = y - t\]

<p>But if the partial derivative of the loss function is not so obvious, we need to take <a href="https://en.wikipedia.org/wiki/Total_derivative">total derivative</a>:</p>

\[\frac{\partial E}{\partial o_j} =\sum_{i\in L}( \frac{\partial E}{\partial net_l}\frac{\partial net_l}{\partial o_j})
 = \sum_{i\in L}(\frac{\partial E}{\partial o_l}\frac{\partial o_l}{\partial net_l}w_{j,l})\]

<p>Where $x_l$ variables are from the next layer (the layer close to th output layer), which are available from previous iterations.</p>

<p>Putting everything together, we have:</p>

<p>\(\frac{\partial E}{\partial w_{i,j}} = \delta_jo_j\)
, where</p>

\[\delta_j = \frac{\partial E}{\partial o_l}\frac{\partial o_l}{\partial net_l} = 
\begin{cases}
(o_j - t_j)o_j(1-o_j), j=outputlayer\\
(\sum_{l\in L}w_{j,l}\delta_l)o_j(1-o_j), otherwise
\end{cases}\]

<p>so</p>

\[\Delta w_{i,j} = -\eta\delta_jo_i\]

<h4 id="regularization">Regularization</h4>

<p>Idea of this section is from <a href="https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a">here</a></p>

<p>Regularization is used to improve the ability generalize on unseen data. Empirical learning of classification is underdetermined because it attempts to infer a function of any x given only data samples. This means models can suffer from over-fitting.</p>

<p>We can apply regularization on loss functions to reduce the effect of over-fitting. the idea is similar to occam razor, which states that when multiple solutions are available to describe a model, simpler ones are more likely to be correct. Regularization tries to simplify the model by reducing coefficients. In order to do this, we just introduce a new term to represent the penalty in the loss function.</p>

<p>For example, we have a residual sum of square loss function</p>

\[RSS = \sum_{i=1}^n(y_i - \beta_0 - \sum_{j=1}^p\beta_jx_{i,j})\]

<p>For Ridge Regression Regularization, we add a term like this</p>

<p>\(RSS + \lambda\sum_{j=1}^p\beta_j^2\)
, which is also called L2 Norm.</p>

<p>In Lasso Regularization, the penalty term is given by:</p>

\[RSS + \lambda\sum_{j=1}^p|\beta_j|\]

<p>Large $\lambda$ indicates high impact of penalty term, so if $\lambda \rightarrow \infty$, all the coefficients are temd to be 0.</p>

<p>In order to find a suitable $\lambda$ value, cross validation can be applied.</p>

<h4 id="optimizers">Optimizers</h4>

<p>Optimizer is a tensorflow module where the optimization algorithms are implemented. It automatically calculate the gradient for each coefficient given data samples from loss and topology of forward connected graph by applying chain rule, which is introduced in Back propagation section.</p>

<p>Whenever optimization is required, the instance of subclass of Optimizer is created, each of them has its implementation of minimize() function.</p>

<p>When the optimizer optimize the model, following operations are taken in order:</p>

<ul>
  <li><strong>Calculating the gradient:</strong> The mothod compute_gradients() is invoked, calculating the gradients following the policy specified.</li>
  <li><strong>Process the gradient:</strong> Clip or weight gradients in this stage to prevent gradient vanish or explode.</li>
  <li><strong>Apply the gradient:</strong> Apply the gradient to the model coefficients after processing.</li>
</ul>

<h3 id="cnn">CNN</h3>
<p>CNN processes images by applying convolution, pooling, fully connect and normalization.</p>
<h4 id="convolution">Convolution</h4>
<p>Applying convolution to an image with a certain amount of conv kernels. We may specift the kernel size, kernel number, stride, padding …</p>
<h4 id="pooling">Pooling</h4>
<p>Pooling layer is applied to convolved image by taking a operation to a S by S grid, for example, max pooling take the max value of the cells.</p>

<h4 id="spatial-pyramid-pooling">Spatial Pyramid Pooling</h4>

<p>Neural networks require a fixed size at output layer, which means the input image must be resized by cropping or wrapping. But such operations may affect some of the features in the image, therefore another method called Spatial Pyramid Pooling(SPP) can be applied after the convolution layer.</p>

<p>SSP layer maintains spatial information by pooling in local spatial bins. Bin sizes are proportional to image input size. The last pooling layer of CNN is replaced by a SPP layer. In each spatial bin, we apply pooling.</p>

<h4 id="fully-connect">Fully connect</h4>
<p>Fully connected layer is used to convert the feature map to the normal neural network layer(logits or propability distribution in classification problems).</p>
<h4 id="convolution-implementation-of-fully-connect">Convolution implementation of fully connect</h4>
<p>The old implementation of fully connected layer is a little bit slow, therefore we can use convolutional implementation. For example, we have a feature map of size $N\times N \times L$, convolution kernels with size $N\times N$ with input channel $L$ can be applied, then the number of kernels is equivalent to the size of fully connected layer.</p>

<h4 id="siamese-net">Siamese Net</h4>

<h2 id="design">Design</h2>

<h3 id="model-and-estimator">Model and estimator</h3>
<p>Estimator is high-level API of tensorflow which helps simplifying building machine learning models. It encapsulates four different operations (available in tf.estimator.ModeKeys):</p>

<ul>
  <li>Training</li>
  <li>Prediction</li>
  <li>Evaluation</li>
  <li>Export for serving</li>
</ul>

<p>To use estimator, we have the define following modules:</p>

<ul>
  <li><strong>Model function</strong> which includes the internal logic of the model, like layers, loss functions and optimizations. It accept following parameters:
    <ul>
      <li>feature (Fed by input function)</li>
      <li>label (Fed by input function)</li>
      <li>mode (instance of tf.estimator.ModeKey)</li>
      <li>params (Other parameters)</li>
    </ul>
  </li>
  <li><strong>Input function</strong> This function feeds data into the model function, act like feed_dict for placeholder in old style APIs. It usually return an Iterator.get_next() which yield the next item of the iterator each time it is evaluated in the session. A recommended implementation is to directly return a tf.data.Dataset instance. The dataset can be pre processed using dataset.map with parse function. For example, it the dataset is a collection of image url, the parse function maps the url to actual image.</li>
</ul>

<h3 id="data-streamer">Data streamer</h3>
<p>In this project, the dataset is in the form of a large image collection, which is sometimes tens or hundreds Gigabytes in size. In this case, we can hardly fit the training dataset into memory, therefore we have to build a pipeline for data streaming. Pipeline does not only help streaming smaller batches of data, but also balance the load between CPU and GPU. The CPU can be used to pre-process and stream the data online, while GPU is in charge of running convolutions and weight updating.</p>

<p>As mentioned before, the dataset is stored in the file system of host, so all we need to do is to build a generator which keeps yielding the path of image files. A very simple version can be built as following:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">def</span> <span class="nf">file_dir_streamer</span><span class="p">(</span><span class="n">image_dir</span><span class="p">):</span>
    <span class="n">file_list</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">walk</span><span class="p">(</span><span class="n">image_dir</span><span class="p">)</span>
    <span class="c1"># root, dir, files
</span>    <span class="n">file_dic</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">file_list</span><span class="p">:</span>
        <span class="n">file_dic</span><span class="p">[</span><span class="n">r</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span> <span class="o">=</span> <span class="n">r</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">same</span> <span class="o">=</span> <span class="bp">True</span>
    <span class="n">keys</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">file_dic</span><span class="p">.</span><span class="n">keys</span><span class="p">())</span>
    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">key</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="n">choice</span><span class="p">(</span><span class="n">keys</span><span class="p">)</span>
            <span class="n">value</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="n">choice</span><span class="p">(</span><span class="n">file_dic</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">key</span><span class="p">))</span>
            <span class="n">path</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
            <span class="n">key_</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="n">choice</span><span class="p">(</span><span class="n">keys</span><span class="p">)</span>
            <span class="n">value_</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="n">choice</span><span class="p">(</span><span class="n">file_dic</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">key_</span><span class="p">))</span>
            <span class="n">path_</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">key_</span><span class="p">,</span> <span class="n">value_</span><span class="p">)</span>
            <span class="c1"># print(path, path_)
</span>            <span class="k">if</span> <span class="n">key_</span> <span class="o">==</span> <span class="n">key</span><span class="p">:</span>
                <span class="n">label</span> <span class="o">=</span> <span class="mf">0.0</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">label</span> <span class="o">=</span> <span class="mf">1.0</span>
            <span class="k">yield</span> <span class="n">path</span><span class="p">,</span> <span class="n">path_</span><span class="p">,</span> <span class="n">label</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="k">pass</span>
</code></pre></div></div>

<p>In the example above, the images are totally randomly selected (Which is not a proper way to train siamese net because this method is likely to give negative image pairs all almost all the time), but it does indicates how is the streamer supposed to work.</p>

<p>It firstly list all the possible file paths by doing a directory tree walk, then yeild three things: two image paths and whether they belongs to the same person.</p>

<p>However, this is not enough because we have to load the actual iamge data from the hard disk, so we have to map the dataset to a new one as following:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">def</span> <span class="nf">_parse_function</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">path_</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
  <span class="n">image_string</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">read_file</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
  <span class="n">image_string_</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">read_file</span><span class="p">(</span><span class="n">path_</span><span class="p">)</span>
  <span class="n">image_decoded</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">image</span><span class="p">.</span><span class="n">decode_jpeg</span><span class="p">(</span><span class="n">image_string</span><span class="p">)</span>
  <span class="n">image_decoded_</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">image</span><span class="p">.</span><span class="n">decode_jpeg</span><span class="p">(</span><span class="n">image_string_</span><span class="p">)</span>
  <span class="n">image_decoded</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">image</span><span class="p">.</span><span class="n">rgb_to_grayscale</span><span class="p">(</span><span class="n">image_decoded</span><span class="p">)</span>
  <span class="n">image_decoded_</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">image</span><span class="p">.</span><span class="n">rgb_to_grayscale</span><span class="p">(</span><span class="n">image_decoded_</span><span class="p">)</span>

  <span class="n">image_resized</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">image</span><span class="p">.</span><span class="n">per_image_standardization</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">image</span><span class="p">.</span><span class="n">resize_images</span><span class="p">(</span><span class="n">image_decoded</span><span class="p">,</span> <span class="p">[</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">]))</span>
  <span class="n">image_resized_</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">image</span><span class="p">.</span><span class="n">per_image_standardization</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">image</span><span class="p">.</span><span class="n">resize_images</span><span class="p">(</span><span class="n">image_decoded_</span><span class="p">,</span> <span class="p">[</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">]))</span>


  <span class="n">out</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">concat</span><span class="p">([</span><span class="n">tf</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">image_resized</span><span class="p">,</span> <span class="p">[</span><span class="mi">16384</span><span class="p">,]),</span> <span class="n">tf</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">image_resized_</span><span class="p">,</span> <span class="p">[</span><span class="mi">16384</span><span class="p">,])],</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">label</span>


<span class="k">def</span> <span class="nf">input_func_gen</span><span class="p">():</span>
    <span class="n">dset</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">Dataset</span><span class="p">.</span><span class="n">from_generator</span><span class="p">(</span>
        <span class="n">generator</span><span class="p">,</span>
        <span class="n">output_types</span><span class="o">=</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">string</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">string</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="n">dset</span> <span class="o">=</span> <span class="n">dset</span><span class="p">.</span><span class="nb">map</span><span class="p">(</span><span class="n">map_func</span><span class="o">=</span><span class="n">_parse_function</span><span class="p">,</span><span class="n">num_parallel_calls</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>



    <span class="k">return</span> <span class="n">dset</span>
</code></pre></div></div>

<p>In the code above, the Dataset instance is firstly instanciated using from_generator method, followed by a mapping method. The parse function is used to map path to the actual image. The parse function accepts the parameters which are exact same as data yielded fro mthe streamer, and return the data of image. The data is the feature parameter of model function</p>



      <footer class="site-footer">
        
          <span class="site-footer-owner"><a href="https://github.com/wylswz/wylswz.github.io">wylswz.github.io</a> is maintained by <a href="https://github.com/wylswz">wylswz</a>.</span>
        
      </footer>
    </main>
  </body>
  <script>

</script>
</html>

